{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Language models example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence', 'that', 'we', 'want', 'to', 'parse', 'and', 'this', 'is', 'done', 'with', 'nltk', 'and', 'python', 'collections']\n"
     ]
    }
   ],
   "source": [
    "# Let's start with some basics first.\n",
    "# Given a sentence, let's see how to perform some counts\n",
    "# with the help of Counters and python dicts\n",
    "\n",
    "from nltk import trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "sentence = 'this is a sentence that we want to parse and this is done with nltk and python collections'\n",
    "sentence = sentence.split()\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None this\n",
      "None this is\n",
      "this is a\n",
      "is a sentence\n",
      "a sentence that\n",
      "sentence that we\n",
      "that we want\n",
      "we want to\n",
      "want to parse\n",
      "to parse and\n",
      "parse and this\n",
      "and this is\n",
      "this is done\n",
      "is done with\n",
      "done with nltk\n",
      "with nltk and\n",
      "nltk and python\n",
      "and python collections\n",
      "python collections None\n",
      "collections None None\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we could get trigrams\n",
    "\n",
    "for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "    print(w1, w2, w3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appear after \"this is\":\n",
      "word: a\n",
      "word: done\n",
      "\n",
      "How many times does \"this is\" occur?\n",
      "2\n",
      "\n",
      "How many times does \"a\" occur after \"this is\"?\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'done': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how we can keep counts\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "    model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Let's see the words that follow \"this is\"\n",
    "print('Words that appear after \"this is\":')\n",
    "for w in model[('this', 'is')]:\n",
    "    print(f'word: {w}')\n",
    "\n",
    "# Let's see how many times \"this is\" occurs\n",
    "print('\\nHow many times does \"this is\" occur?')\n",
    "print(sum(model[('this', 'is')].values()))\n",
    "\n",
    "# Let's see how many time \"a\" occurs after \"this is\"\n",
    "print('\\nHow many times does \"a\" occur after \"this is\"?')\n",
    "print(model[('this', 'is')]['a'])\n",
    "\n",
    "dict(model[('this', 'is')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put everything together and use a corpus from project Gutenberg\n",
    "# which is provided directly by NLTK\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in gutenberg.sents('shakespeare-macbeth.txt'):\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "        \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ['Scoena', 'Prima', '.'], ['Thunder', 'and', 'Lightning', '.'], ['Enter', 'three', 'Witches', '.'], ['1', '.'], ['When', 'shall', 'we', 'three', 'meet', 'againe', '?'], ['In', 'Thunder', ',', 'Lightning', ',', 'or', 'in', 'Raine', '?'], ['2', '.'], ['When', 'the', 'Hurley', '-', 'burley', \"'\", 's', 'done', ',', 'When', 'the', 'Battaile', \"'\", 's', 'lost', ',', 'and', 'wonne'], ['3', '.'], ['That', 'will', 'be', 'ere', 'the', 'set', 'of', 'Sunne'], ['1', '.'], ['Where', 'the', 'place', '?'], ['2', '.'], ['Vpon', 'the', 'Heath'], ['3', '.'], ['There', 'to', 'meet', 'with', 'Macbeth'], ['1', '.'], ['I', 'come', ',', 'Gray', '-', 'Malkin'], ['All', '.'], ['Padock', 'calls', 'anon', ':', 'faire', 'is', 'foule', ',', 'and', 'foule', 'is', 'faire', ',', 'Houer', 'through', 'the', 'fogge', 'and', 'filthie', 'ayre', '.'], ['Exeunt', '.'], ['Scena', 'Secunda', '.'], ['Alarum', 'within', '.'], ['Enter', 'King', 'Malcome', ',', 'Donalbaine', ',', 'Lenox', ',', 'with', 'attendants', ',', 'meeting', 'a', 'bleeding', 'Captaine', '.'], ['King', '.'], ['What', 'bloody', 'man', 'is', 'that', '?'], ['he', 'can', 'report', ',', 'As', 'seemeth', 'by', 'his', 'plight', ',', 'of', 'the', 'Reuolt', 'The', 'newest', 'state'], ['Mal', '.'], ['This', 'is', 'the', 'Serieant', ',', 'Who', 'like', 'a', 'good', 'and', 'hardie', 'Souldier', 'fought', \"'\", 'Gainst', 'my', 'Captiuitie', ':', 'Haile', 'braue', 'friend', ';', 'Say', 'to', 'the', 'King', ',', 'the', 'knowledge', 'of', 'the', 'Broyle', ',', 'As', 'thou', 'didst', 'leaue', 'it'], ['Cap', '.'], ['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that', 'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The', 'mercilesse', 'Macdonwald', '(', 'Worthie', 'to', 'be', 'a', 'Rebell', ',', 'for', 'to', 'that', 'The', 'multiplying', 'Villanies', 'of', 'Nature', 'Doe', 'swarme', 'vpon', 'him', ')', 'from', 'the', 'Westerne', 'Isles', 'Of', 'Kernes', 'and', 'Gallowgrosses', 'is', 'supply', \"'\", 'd', ',', 'And', 'Fortune', 'on', 'his', 'damned', 'Quarry', 'smiling', ',', 'Shew', \"'\", 'd', 'like', 'a', 'Rebells', 'Whore', ':', 'but', 'all', \"'\", 's', 'too', 'weake', ':', 'For', 'braue', 'Macbeth', '(', 'well', 'hee', 'deserues', 'that', 'Name', ')', 'Disdayning', 'Fortune', ',', 'with', 'his', 'brandisht', 'Steele', ',', 'Which', 'smoak', \"'\", 'd', 'with', 'bloody', 'execution', '(', 'Like', 'Valours', 'Minion', ')', 'caru', \"'\", 'd', 'out', 'his', 'passage', ',', 'Till', 'hee', 'fac', \"'\", 'd', 'the', 'Slaue', ':', 'Which', 'neu', \"'\", 'r', 'shooke', 'hands', ',', 'nor', 'bad', 'farwell', 'to', 'him', ',', 'Till', 'he', 'vnseam', \"'\", 'd', 'him', 'from', 'the', 'Naue', 'toth', \"'\", 'Chops', ',', 'And', 'fix', \"'\", 'd', 'his', 'Head', 'vpon', 'our', 'Battlements'], ['King', '.'], ['O', 'valiant', 'Cousin', ',', 'worthy', 'Gentleman'], ['Cap', '.'], ['As', 'whence', 'the', 'Sunne', \"'\", 'gins', 'his', 'reflection', ',', 'Shipwracking', 'Stormes', ',', 'and', 'direfull', 'Thunders', ':', 'So', 'from', 'that', 'Spring', ',', 'whence', 'comfort', 'seem', \"'\", 'd', 'to', 'come', ',', 'Discomfort', 'swells', ':', 'Marke', 'King', 'of', 'Scotland', ',', 'marke', ',', 'No', 'sooner', 'Iustice', 'had', ',', 'with', 'Valour', 'arm', \"'\", 'd', ',', 'Compell', \"'\", 'd', 'these', 'skipping', 'Kernes', 'to', 'trust', 'their', 'heeles', ',', 'But', 'the', 'Norweyan', 'Lord', ',', 'surueying', 'vantage', ',', 'With', 'furbusht', 'Armes', ',', 'and', 'new', 'supplyes', 'of', 'men', ',', 'Began', 'a', 'fresh', 'assault'], ['King', '.'], ['Dismay', \"'\", 'd', 'not', 'this', 'our', 'Captaines', ',', 'Macbeth', 'and', 'Banquoh', '?'], ['Cap', '.'], ['Yes', ',', 'as', 'Sparrowes', ',', 'Eagles', ';', 'Or', 'the', 'Hare', ',', 'the', 'Lyon', ':', 'If', 'I', 'say', 'sooth', ',', 'I', 'must', 'report', 'they', 'were', 'As', 'Cannons', 'ouer', '-', 'charg', \"'\", 'd', 'with', 'double', 'Cracks', ',', 'So', 'they', 'doubly', 'redoubled', 'stroakes', 'vpon', 'the', 'Foe', ':', 'Except', 'they', 'meant', 'to', 'bathe', 'in', 'reeking', 'Wounds', ',', 'Or', 'memorize', 'another', 'Golgotha', ',', 'I', 'cannot', 'tell', ':', 'but', 'I', 'am', 'faint', ',', 'My', 'Gashes', 'cry', 'for', 'helpe'], ['King', '.'], ['So', 'well', 'thy', 'words', 'become', 'thee', ',', 'as', 'thy', 'wounds', ',', 'They', 'smack', 'of', 'Honor', 'both', ':', 'Goe', 'get', 'him', 'Surgeons', '.'], ['Enter', 'Rosse', 'and', 'Angus', '.'], ['Who', 'comes', 'here', '?'], ['Mal', '.'], ['The', 'worthy', 'Thane', 'of', 'Rosse'], ['Lenox', '.'], ['What', 'a', 'haste', 'lookes', 'through', 'his', 'eyes', '?'], ['So', 'should', 'he', 'looke', ',', 'that', 'seemes', 'to', 'speake', 'things', 'strange']]\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.sents('shakespeare-macbeth.txt')[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faint': 0.034482758620689655,\n",
       " 'Thane': 0.06896551724137931,\n",
       " 'fed': 0.034482758620689655,\n",
       " 'his': 0.034482758620689655,\n",
       " 'settled': 0.034482758620689655,\n",
       " 'afraid': 0.06896551724137931,\n",
       " 'one': 0.034482758620689655,\n",
       " 'recklesse': 0.034482758620689655,\n",
       " 'cabin': 0.034482758620689655,\n",
       " 'a': 0.034482758620689655,\n",
       " 'bent': 0.034482758620689655,\n",
       " 'in': 0.06896551724137931,\n",
       " 'for': 0.034482758620689655,\n",
       " 'call': 0.034482758620689655,\n",
       " 'so': 0.034482758620689655,\n",
       " 'not': 0.06896551724137931,\n",
       " 'perfect': 0.034482758620689655,\n",
       " 'too': 0.034482758620689655,\n",
       " 'yong': 0.034482758620689655,\n",
       " 'as': 0.034482758620689655,\n",
       " 'yet': 0.034482758620689655,\n",
       " 'truly': 0.034482758620689655,\n",
       " ',': 0.034482758620689655,\n",
       " 'sure': 0.034482758620689655,\n",
       " 'sick': 0.034482758620689655}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model['I', 'am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am yong , but can perceiue no truth in your state of Man , That I may tell pale - hearted Feare , and Macbeth , Or weare it on my Sword ; yet all this , and know How tender ' tis one\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# starting words\n",
    "text = [\"I\", \"am\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "\n",
    "    for word in model[tuple(text[-2:])].keys():\n",
    "        accumulator += model[tuple(text[-2:])][word]\n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "\n",
    "print(' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimates for test data:\n",
      "\n",
      "MLE Estimates for sentence 0: [(('thunder', ('<s>',)), 0.16666666666666666), (('and', ('thunder',)), 1.0), (('lightning', ('and',)), 1.0), (('</s>', ('lightning',)), 1.0)]\n",
      "\n",
      "MLE Estimates for sentence 1: [(('through', ('<s>',)), 0.0), (('his', ('through',)), 0), (('eyes', ('his',)), 0), (('</s>', ('eyes',)), 1.0)]\n",
      "\n",
      "MLE Estimates for sentence 2: [(('i', ('<s>',)), 0.16666666666666666), (('haue', ('i',)), 0.5), (('the', ('haue',)), 0.5), (('king', ('the',)), 0.3333333333333333), (('</s>', ('king',)), 1.0)]\n",
      "\n",
      "Perplexities:\n",
      "PP(Thunder and lightning):1.5650845800732873\n",
      "PP(through his eyes):inf\n",
      "PP(I haue the king):2.352158045049347\n"
     ]
    }
   ],
   "source": [
    "# Another example:\n",
    "# Fit directly an MLE \n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "train_sentences = ['Thunder and Lightning',\n",
    "                   'Enter three Witches',\n",
    "                   'I am faint',\n",
    "                   'God saue the King',\n",
    "                   'Looke what I haue here',\n",
    "                   'Here the lies haue the eyes'\n",
    "                  ]\n",
    "\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
    "\n",
    "n = 2 # Highest n-gram order for the Maximul Likelihood Estimator\n",
    "\n",
    "# Prepare training data:\n",
    "# Use bigrams, and mark the start and end of the sentence\n",
    "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "words = [word for sent in tokenized_text for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "padded_vocab = Vocabulary(words)\n",
    "\n",
    "# Fit model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "# Use 3 test sentences\n",
    "# The first sentence appears in the dataset\n",
    "# The other two do not appear in the dataset. \n",
    "# However, for the 3rd sentence there are similar bigrams in the training set\n",
    "test_sentences = ['Thunder and lightning', 'through his eyes', 'I haue the king']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
    "\n",
    "print('MLE estimates for test data:')\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i,test in enumerate(test_data):\n",
    "    print (f'\\nMLE Estimates for sentence {i}:', [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "print('\\nPerplexities:')\n",
    "# Reset the test_data, since the generator has been exhausted\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
